model:
  base_model: "meta-llama/Llama-3.1-8B"
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.1

data:
  dataset: "glaiveai/glaive-function-calling-v2"
  split: "train"
  num_examples: 5
  output_dir: "outputs"

ppo:
  clip_epsilon: 0.15
  learning_rate: 3e-5
  batch_size: 32
  epochs_per_update: 4
  entropy_coef: 0.001
  value_loss_coef: 0.5
  max_grad_norm: 1.0
  target_kl: 0.02
  gamma: 0.99
  gae_lambda: 0.95

environment:
  max_steps_per_episode: 8
  step_reward_correct: 0.3
  step_reward_incorrect: -0.1
  final_reward_success: 1.0
  final_reward_fail: 0.0

checkpoint:
  save_name: "toolbench_ppo"
